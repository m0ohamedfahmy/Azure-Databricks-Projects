{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3431d6ad-7e9b-4980-a7c0-1389f721f25f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "def add_ingestion_date(input_df):\n",
    "    \"\"\"\n",
    "    Adds an ingestion date column to the input DataFrame.\n",
    "\n",
    "    This function adds a new column named \"ingest_date\" to the provided DataFrame. \n",
    "    The new column contains the current timestamp at the time of the function call, \n",
    "    representing the date and time when the data was ingested.\n",
    "\n",
    "    Args:\n",
    "    input_df (DataFrame): The input DataFrame to which the ingestion date column will be added.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame with an additional column \"ingest_date\" containing the current timestamp.\n",
    "    \"\"\"\n",
    "    output=input_df.withColumn(\"ingest_date\",current_timestamp())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba23475-a1fe-4b96-bfb1-1a592a769bc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def re_arrange_partition_column(input_df, partition_column):\n",
    "  \"\"\"\n",
    "  Rearranges the columns of a DataFrame to place a specified partition column at the end.\n",
    "\n",
    "  This function takes an input DataFrame and a partition column name as arguments. It then reorders the columns in\n",
    "  the DataFrame so that the specified partition column is moved to the end, while preserving the order of the other columns.\n",
    "\n",
    "  Parameters:\n",
    "  input_df (DataFrame): The input DataFrame whose columns are to be rearranged.\n",
    "  partition_column (str): The name of the column that should be moved to the end of the DataFrame.\n",
    "\n",
    "  Returns:\n",
    "  DataFrame: A new DataFrame with the columns reordered such that the specified partition column is at the end.\n",
    "  \"\"\"\n",
    "  column_list = []\n",
    "  for column_name in input_df.schema.names:\n",
    "    if column_name != partition_column:\n",
    "      column_list.append(column_name)\n",
    "  column_list.append(partition_column)\n",
    "  output_df = input_df.select(column_list)\n",
    "  return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c32f5356-d228-4170-a291-b9d39ef07719",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def overwrite_partition(input_df, db_name, table_name, partition_column):\n",
    "  \"\"\"\n",
    "  Merges data from the input DataFrame into a Delta table. If the Delta table already exists, performs a merge operation\n",
    "  based on the specified merge condition. If the table does not exist, creates a new Delta table from the input DataFrame.\n",
    "  \n",
    "  Parameters:\n",
    "  - input_df (DataFrame): The DataFrame containing the data to be merged into the Delta table.\n",
    "  - db_name (str): The name of the database where the Delta table resides or will be created.\n",
    "  - table_name (str): The name of the Delta table to merge data into or create.\n",
    "  - folder_path (str): The path to the folder where the Delta table data is stored.\n",
    "  - merge_condition (str): The SQL-like condition used to match records between the input DataFrame and the Delta table.\n",
    "  - partition_column (str): The column used to partition the Delta table when it is created.\n",
    "    \n",
    "  Returns:\n",
    "  None\n",
    "  \"\"\"\n",
    "  output_df = re_arrange_partition_column(input_df, partition_column)\n",
    "  spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\n",
    "  if (spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\")):\n",
    "    output_df.write.mode(\"overwrite\").insertInto(f\"{db_name}.{table_name}\")\n",
    "  else:\n",
    "    output_df.write.mode(\"overwrite\").partitionBy(partition_column).format(\"parquet\").saveAsTable(f\"{db_name}.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5af0e5c-ce3c-4c5f-9df0-a0efb0ef3bc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "def merge_delta_data(input_df, db_name, table_name, folder_path, merge_condition, partition_column):\n",
    "  \"\"\"\n",
    "  Merges data from the input DataFrame into a Delta table. If the Delta table already exists, performs a merge operation\n",
    "  based on the specified merge condition. If the table does not exist, creates a new Delta table from the input DataFrame.\n",
    "    \n",
    "  Parameters:\n",
    "  - input_df (DataFrame): The DataFrame containing the data to be merged into the Delta table.\n",
    "  - db_name (str): The name of the database where the Delta table resides or will be created.\n",
    "  - table_name (str): The name of the Delta table to merge data into or create.\n",
    "  - folder_path (str): The path to the folder where the Delta table data is stored.\n",
    "  - merge_condition (str): The SQL-like condition used to match records between the input DataFrame and the Delta table.\n",
    "  - partition_column (str): The column used to partition the Delta table when it is created.\n",
    "    \n",
    "  \"\"\"\n",
    "  spark.conf.set(\"spark.databricks.optimizer.dynamicPartitionPruning\",\"true\")\n",
    "  if (spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\")):\n",
    "    deltaTable = DeltaTable.forPath(spark, f\"{folder_path}/{table_name}\")\n",
    "    deltaTable.alias(\"tgt\").merge(\n",
    "        input_df.alias(\"src\"),\n",
    "        merge_condition) \\\n",
    "      .whenMatchedUpdateAll()\\\n",
    "      .whenNotMatchedInsertAll()\\\n",
    "      .execute()\n",
    "  else:\n",
    "    input_df.write.mode(\"overwrite\").partitionBy(partition_column).format(\"delta\").saveAsTable(f\"{db_name}.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88feb416-ba41-4bb2-a169-39c432768b43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_race_year_list(input_df,column_name):\n",
    "    \"\"\"\n",
    "    Extracts a list of distinct values from a specified column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    input_df (DataFrame): The input DataFrame from which to extract distinct values.\n",
    "    column_name (str): The name of the column to extract distinct values from.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing distinct values from the specified column.\n",
    "    \"\"\"\n",
    "    output_df=input_df.select(column_name).distinct().collect()\n",
    "    race_year_list=[year.column_name for year in output_df ]\n",
    "    return race_year_list"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Common_functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
